#!/usr/bin/env bash

# VLP - Stage1 (Pretrained)
GPU_DEVICE="0,1,2,3,4,5,6"
main_ip="143.248.49.62"
main_port=11111
machine_rank=0
n_mach=2
length=${#GPU_DEVICE}
n_gpu=$(((length+1)/2))
n_proc=$((n_gpu*n_mach))
image_bath=4
test_batch=1

# 143.248.49.62
# 53773

# 143.248.49.126
# 57646

# 143.248.49.128
# 53277

# CuLLaVOPipeline
CUDA_VISIBLE_DEVICES=$GPU_DEVICE accelerate launch --config_file configs/accel/node_ddp_accel.yaml \
                    --num_machines=$n_mach \
                    --num_processes=$n_proc \
                    --main_process_ip=$main_ip \
                    --main_process_port=$main_port \
                    --machine_rank=$machine_rank \
                    entry.py train \
                    --conf_files configs/cullavo_step1.yaml \
                    --overrides \
                    WANDB True \
                    PIPELINE CuLLaVOPipeline\
                    COCO.TRAIN.BATCH_SIZE_TOTAL $((n_gpu * image_bath)) \
                    COCO.TRAIN.BATCH_SIZE_PER_GPU $image_bath \
                    COCO.TEST.BATCH_SIZE_TOTAL $((n_gpu * test_batch)) \
                    VLP.TRAIN.BATCH_SIZE_TOTAL $((n_gpu * image_bath)) \
                    VLP.TRAIN.BATCH_SIZE_PER_GPU $image_bath \
                    VLP.TEST.BATCH_SIZE_TOTAL $((n_gpu * test_batch)) \
                    REF.TEST.BATCH_SIZE_TOTAL $n_gpu \
                    VQA.TEST.BATCH_SIZE_TOTAL $n_gpu \